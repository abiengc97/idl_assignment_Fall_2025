{
  "parameter_count": 16797968,
  "model_architecture": "EncoderDecoderTransformer(\n  (source_embedding): SpeechEmbedding(\n    (cnn): Conv2DSubsampling(\n      (conv): Sequential(\n        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 1))\n        (1): GELU(approximate='none')\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n        (3): GELU(approximate='none')\n      )\n      (linear_out): Linear(in_features=19456, out_features=256, bias=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (positional_encoding): PositionalEncoding()\n  (dropout): Dropout(p=0.05, inplace=False)\n  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (final_linear): Linear(in_features=256, out_features=5000, bias=True)\n  (enc_layers): ModuleList(\n    (0-3): 4 x SelfAttentionEncoderLayer(\n      (self_attn): SelfAttentionLayer(\n        (mha): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n      (ffn): FeedForwardLayer(\n        (ffn): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.05, inplace=False)\n          (3): Linear(in_features=1024, out_features=256, bias=True)\n        )\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n    )\n  )\n  (dec_layers): ModuleList(\n    (0-3): 4 x CrossAttentionDecoderLayer(\n      (self_attn): SelfAttentionLayer(\n        (mha): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n      (cross_attn): CrossAttentionLayer(\n        (mha): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n      (ffn): FeedForwardLayer(\n        (ffn): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.05, inplace=False)\n          (3): Linear(in_features=1024, out_features=256, bias=True)\n        )\n        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n    )\n  )\n  (target_embedding): Embedding(5000, 256)\n  (encoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (ctc_head): Sequential(\n    (0): Linear(in_features=256, out_features=5000, bias=True)\n    (1): LogSoftmax(dim=-1)\n  )\n)"
}