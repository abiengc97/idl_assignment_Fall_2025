{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1P2: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zcx2lHRylKP"
      },
      "source": [
        "# Schedule:\n",
        "- Checkpoint Submission (DUE 5 September 2025 @ 11:59PM EST)\n",
        "- Final Submission (DUE 19 September 2025 @ 11:59PM EST | Slack Deadline is 26 September 2025 @ 11:59PM EST)\n",
        "- Code Submission (DUE 21 September 2025 @ 11:59PM EST OR Day-of Slack submission)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcf4nWStyj1g"
      },
      "source": [
        "## Requirement Acknowledgement\n",
        "Setting the below flag to True indicates full understanding and acceptance of the following:\n",
        "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
        "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
        "3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
        "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
        "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
        "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
        "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
        "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
        "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zay-09kmxqjQ"
      },
      "outputs": [],
      "source": [
        "ACKNOWLEDGED = False #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vp3N7qr_5V"
      },
      "source": [
        "# Dataset Description\n",
        "\n",
        "Let's start by understanding the dataset for this homework.\n",
        "\n",
        "Our data consists of 3 folders (train-clean-100, dev-clean and test-clean). The training and validation datasets (train-clean-100 and dev-clean) each contain 2 subfolders (mfcc and transcript). The 'mfcc' subfolder contains mel spectrograms (explained below and in writeup), while the 'transcript' subfolder contains their corresponding transcripts. However, the test dataset (test-clean) contains only the 'mfcc' subfolder without the corresponding transcripts, which will later be predicted by your model.\n",
        "\n",
        "\n",
        "## 1. Audio Representation.\n",
        "The 'mfcc' subfolders contain many `*.npy` files of mel spectrograms. .npy files are used to store numpy arrays.\n",
        "\n",
        "Each .npy file represents a short speech recording. For example, one recording might be someone saying, \"This is the age of AI.\" This recording is converted into a mel spectrogram, which is used to represent all forms of audio signals in a computer. Such representation is important in signal and speech processing tasks, especially in machine learning.\n",
        "\n",
        "Compared to raw audio, mel spectrograms are better for speech processing because they capture both the timing and the frequencies of the sound. At each moment in time, they show which frequencies are present in the sound. This makes it easier for computers to understand and process speech.\n",
        "\n",
        "When converting raw audio to spectrograms, you do not process the whole audio at once. Instead, you process small frames at a time as you stride over the entire audio length. This means that if you have an audio file of 100 seconds, you may decide to process 10 seconds at a time, striding by one second. In this case, the frame size is 10 seconds. The frame size and the number of timesteps (seconds, milliseconds, etc.) depend on individual choice.\n",
        "\n",
        "When processing each frame, you extract a number of features that represent that frame's audio. For instance, in the audio recording of \"This is the age of AI,\" the frame corresponding to \"AI\" will have features that represent how \"AI\" is pronounced, the vocal tract, and the effect of the environment in which it was recorded. For clarity, when we say features, you should think of columns. One feature/column may have information about the gender of the person who made the speech. Another may have information about the age of the person. Another may have information about the environment where the speech was recorded. Basically, the main properties that make up a speech are encoded in those features, which combine in some way to make the audio.\n",
        "\n",
        "Since we want to recognize the word as it was pronounced despite the environment and other variabilities, we usually normalize to eliminate or minimize such effects.\n",
        "\n",
        "Our spectrograms contain 28 features. Essentially, the number of features may be different. They may depend on how the raw audio data was converted into mel spectrograms.\n",
        "\n",
        "## 2. Transcripts\n",
        "Remember where we mentioned frames? Well, in our dataset, audio frames have corresponding target transcripts. For instance the abbreviation \"AI\", in our example above, if present in the recordings, will have transcripts: /eɪ aɪ/. This means that you will have two frames one for  /eɪ/ and another for /aɪ/.\n",
        "\n",
        "This way of representing pronounciation in text form is called ***phonetic transcription***, \"the conversion of spoken words the way they are pronounced instead of how they are written\"[[link]](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://krisp.ai/blog/phonetic-transcription/%23:~:text%3Dphonetic%2520transcriptions%2520done.-,What%2520are%2520Phonetic%2520Transcriptions%253F,verbatim%2520to%2520intelligent%2520verbatim%2520transcriptions.&ved=2ahUKEwiV6LO6hrSHAxUKSvEDHcvwAAsQFnoECB0QAw&usg=AOvVaw0VqoWceOzdVwe-AvdyyWqJ). In this case letters 'A' and 'I' are pronounce /eɪ/ and /aɪ/, respectively. Both letters in different words may be pronounced differently.\n",
        "\n",
        "The produced representation of the speech is referred to as phonemes. Various .npy files that contain recordings of the sentence **\"This is the age of AI.\"** would map to **\"/ðɪs ɪz ðə eɪdʒ əv eɪ aɪ/.\"** The phonemes representation for **Chelsea sucks** would be **/ˈtʃɛl.si sʌks/**\n",
        "\n",
        "Going inside the .npy files. Each .npy file contains vectors which have 28 features/dimensions/columns. The number of vectors in the file corresponds to the number of frames in the recording. And each single frame has a corresponding phoneme in the transcript.\n",
        "\n",
        "For instance the .npy file for \"This is the age of AI\" --> \"/ðɪs ɪz ðə eɪdʒ əv eɪ aɪ/\" might have 13 frames (13 vectors):\n",
        "\n",
        "- /ðɪs/ has 3 phonemes: /ð/, /ɪ/, /s/  \n",
        "- /ɪz/ has 2 phonemes: /ɪ/, /z/\n",
        "- /ðə/ has 2 phonemes: /ð/, /ə/\n",
        "- /eɪdʒ/ has 2 phonemes: /eɪ/, /dʒ/\n",
        "- /əv/ has 2 phonemes: /ə/, /v/\n",
        "- /eɪ aɪ/ has 2 phonemes: /eɪ/, /aɪ/\n",
        "\n",
        "**Chelsea sucks** --> **/ˈtʃɛl.si sʌks/** might have 8 frames (8 vectors):\n",
        "\n",
        "- /ˈtʃɛl.si/ has 4 phonemes: /tʃ/, /ɛ/, /l/, /si/\n",
        "- /sʌks/ has 4 phonemes: /s/, /ʌ/, /k/, /s/\n",
        "\n",
        "Note that recordings of different sentences may have different number of frames.\n",
        "\n",
        "The model you will produce must take a vector of a particular frame and predict the frame's transcript as accurately as possible.\n",
        "\n",
        "Therefore, the **__getitem__** method of your dataset class must return a 28 dimensional vector of a particular frame and its corresponding phoneme transcript.\n",
        "\n",
        "This means that, while you are doing your data preprocessing in the **__init__** method, you need stack all vectors from all recordings on top of each other. You must do this for all transcripts as well and remember to ensure the correspondance between frames and their phoneme mapping is maintained.\n",
        "\n",
        "For our dataset of two samples above, if you stack the recordings together, you get:\n",
        "\n",
        "\n",
        "| Frame | Feature 1 | Feature 2 | ... | Feature 28 | Phoneme |\n",
        "|-------|-----------|-----------|-----|------------|---------|\n",
        "| 0     | v0_1      | v0_2      | ... | v0_28      | /ð/     |\n",
        "| 1     | v1_1      | v1_2      | ... | v1_28      | /ɪ/     |\n",
        "| 2     | v2_1      | v2_2      | ... | v2_28      | /s/     |\n",
        "| 3     | v3_1      | v3_2      | ... | v3_28      | /ɪ/     |\n",
        "| 4     | v4_1      | v4_2      | ... | v4_28      | /z/     |\n",
        "| 5     | v5_1      | v5_2      | ... | v5_28      | /ð/     |\n",
        "| 6     | v6_1      | v6_2      | ... | v6_28      | /ə/     |\n",
        "| 7     | v7_1      | v7_2      | ... | v7_28      | /eɪ/    |\n",
        "| 8     | v8_1      | v8_2      | ... | v8_28      | /dʒ/    |\n",
        "| 9     | v9_1      | v9_2      | ... | v9_28      | /ə/     |\n",
        "| 10    | v10_1     | v10_2     | ... | v10_28     | /v/     |\n",
        "| 11    | v11_1     | v11_2     | ... | v11_28     | /eɪ/    |\n",
        "| 12    | v12_1     | v12_2     | ... | v12_28     | /aɪ/    |\n",
        "| 13    | v13_1     | v13_2     | ... | v13_28     | /tʃ/    |\n",
        "| 14    | v14_1     | v14_2     | ... | v14_28     | /ɛ/     |\n",
        "| 15    | v15_1     | v15_2     | ... | v15_28     | /l/     |\n",
        "| 16    | v16_1     | v16_2     | ... | v16_28     | /si/     |\n",
        "| 17    | v17_1     | v17_2     | ... | v17_28     | /s/     |\n",
        "| 18    | v18_1     | v18_2     | ... | v18_28     | /ʌ/     |\n",
        "| 19    | v19_1     | v19_2     | ... | v19_28     | /k/     |\n",
        "| 20    | v20_1     | v20_2     | ... | v20_28     | /s/     |\n",
        "\n",
        "\n",
        "So, if you pass index 5 to **__getitem__**, you will get back vector v5 (v5_1, v5_2, ..., v5_28) and transcript **/ð/**. Ideally, if you have a well trained model, it should take v5 and return **/ð/**. And the call to **__len__** would return 21 which the training loop would use to go through the whole dataset.\n",
        "\n",
        "## Context\n",
        "\n",
        "In the dataset we are using, a few millisecs were used to convert raw audio to mel spectrogram and extract the 28 features.\n",
        "Since each vector represents only a few millisecs of speech, it may not be sufficient to feed only a single vector into the network at a time. Instead, it may be useful to provide the network with some “context” of size K around each vector in terms of additional vectors from the speech input.\n",
        "\n",
        "Concretely, a context of size 3 would mean that we provide an input of size (7, 28) to the network - the size 7 can be explained as: the vector to predict the label for, 3 vectors preceding this vector, and 3 vectors following it. It is worth thinking about how you would handle providing context before one of the first K frames of an utterance or after one of the last K frames.\n",
        "\n",
        "There are several ways to implement this, but you could try the simplest one:\n",
        "- Concatenating all utterances and padding with K 0-valued vectors before and after the resulting matrix\n",
        "\n",
        "If you use a context of 3 on the above table, you get the following table:\n",
        "\n",
        "| Frame | Feature 1 | Feature 2 | ... | Feature 28 | Phoneme | Context Vectors |\n",
        "|-------|-----------|-----------|-----|------------|---------|----------------|\n",
        "| 0     | v0_1      | v0_2      | ... | v0_28      | /ð/     | [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3 |\n",
        "| 1     | v1_1      | v1_2      | ... | v1_28      | /ɪ/     | [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3, v4 |\n",
        "| 2     | v2_1      | v2_2      | ... | v2_28      | /s/     | [0, 0, 0, ..., 0] (Padding), v0, v1, v2, v3, v4, v5 |\n",
        "| 3     | v3_1      | v3_2      | ... | v3_28      | /ɪ/     | v0, v1, v2, v3, v4, v5, v6 |\n",
        "| 4     | v4_1      | v4_2      | ... | v4_28      | /z/     | v1, v2, v3, v4, v5, v6, v7 |\n",
        "| 5     | v5_1      | v5_2      | ... | v5_28      | /ð/     | v2, v3, v4, v5, v6, v7, v8 |\n",
        "| 6     | v6_1      | v6_2      | ... | v6_28      | /ə/     | v3, v4, v5, v6, v7, v8, v9 |\n",
        "| 7     | v7_1      | v7_2      | ... | v7_28      | /eɪ/    | v4, v5, v6, v7, v8, v9, v10 |\n",
        "| 8     | v8_1      | v8_2      | ... | v8_28      | /dʒ/    | v5, v6, v7, v8, v9, v10, v11 |\n",
        "| 9     | v9_1      | v9_2      | ... | v9_28      | /ə/     | v6, v7, v8, v9, v10, v11, v12 |\n",
        "| 10    | v10_1     | v10_2     | ... | v10_28     | /v/     | v7, v8, v9, v10, v11, v12, v13 |\n",
        "| 11    | v11_1     | v11_2     | ... | v11_28     | /eɪ/    | v8, v9, v10, v11, v12, v13, v14 |\n",
        "| 12    | v12_1     | v12_2     | ... | v12_28     | /aɪ/    | v9, v10, v11, v12, v13, v14, v15 |\n",
        "| 13    | v13_1     | v13_2     | ... | v13_28     | /tʃ/    | v10, v11, v12, v13, v14, v15, v16 |\n",
        "| 14    | v14_1     | v14_2     | ... | v14_28     | /ɛ/     | v11, v12, v13, v14, v15, v16, v17 |\n",
        "| 15    | v15_1     | v15_2     | ... | v15_28     | /l/     | v12, v13, v14, v15, v16, v17, v18 |\n",
        "| 16    | v16_1     | v16_2     | ... | v16_28     | /s/     | v13, v14, v15, v16, v17, v18, v19 |\n",
        "| 17    | v17_1     | v17_2     | ... | v17_28     | /i/     | v14, v15, v16, v17, v18, v19, v20 |\n",
        "| 18    | v18_1     | v18_2     | ... | v18_28     | /s/     | v15, v16, v17, v18, v19, v20, v21 |\n",
        "| 19    | v19_1     | v19_2     | ... | v19_28     | /ʌ/     | v16, v17, v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding) |\n",
        "| 20    | v20_1     | v20_2     | ... | v20_28     | /k/     | v17, v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding) |\n",
        "| 21    | v21_1     | v21_2     | ... | v21_28     | /s/     | v18, v19, v20, v21, [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding), [0, 0, 0, ..., 0] (Padding) |\n",
        "\n",
        "\n",
        "Now, if you want to predict the output of vector v5, you won't just pass vector v5 alone. You will concatenate 3 vectors before it and 3 vectors after, which makes it 7 vectors ([v2, v3, v4, v5, v6, v7, v8 ]) . This needs to be reflected in your **__getitem__** method. Meaning it should return an array of shape (7, 28), in this example.\n",
        "\n",
        "Hence your model is going to be taking a tensor (array) of shape (7, 28) in this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwYu9sSUnSho"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummaryX==1.1.0 wandb --quiet\n",
        "!pip install torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI4qfx7tiBZt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import json\n",
        "import bisect\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "import yaml\n",
        "import torchaudio.transforms as tat\n",
        "import torchaudio\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqsFLqa6rCuc"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z23Nag1jq_yA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDgzbbXsf9og"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPBUd7Cnl-Rx"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"YOUR_USER_NAME\" #TODO: Put your kaggle username here\n",
        "KAGGLE_API_KEY = \"YOUR_API_KEY\" # TODO: Put your kaggle api key here\n",
        "\n",
        "!mkdir /root/.kaggle\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"username\": KAGGLE_USERNAME,\n",
        "        \"key\": KAGGLE_API_KEY\n",
        "    }, f)\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsVxXvJy4QSA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if2Somqfbje1"
      },
      "outputs": [],
      "source": [
        "# commands to download data from kaggle\n",
        "!kaggle competitions download -c 11785-hw-1-p-2-fall-25\n",
        "\n",
        "# Unzip downloaded data\n",
        "!unzip -qo /content/11785-hw-1-p-2-fall-25.zip -d '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNacQ8bpt9nw"
      },
      "source": [
        "# Parameters Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_B07OzYrXj"
      },
      "outputs": [],
      "source": [
        "%%writefile config.yaml\n",
        "epochs        : 5,\n",
        "batch_size    : 1024,\n",
        "context       : 20,\n",
        "init_lr       : 1e-3,\n",
        "architecture  : 'very-low-cutoff'\n",
        "# Add more as you need them - e.g dropout values, weight decay, scheduler parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GdaOmWreYrXj"
      },
      "outputs": [],
      "source": [
        "with open(\"config.yaml\") as file:\n",
        "    config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWGmtoOfYrXj"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYeyFHQ1yRi4"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HYU4NAH65dSb"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = NotImplemented\n",
        "        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.transcript_dir = NotImplemented\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = NotImplemented\n",
        "        # TODO: List files in self.transcript_dir using os.listdir in sorted order\n",
        "        transcript_names    = NotImplemented\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = NotImplemented\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = NotImplemented # Remove [SOS] and [EOS] from the transcript\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that\n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
        "        self.mfccs          = NotImplemented\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that\n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts    = NotImplemented\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Take some time to think about what we have done.\n",
        "        # self.mfcc is an array of the format (Frames x Features).\n",
        "        # Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is.\n",
        "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "        self.mfccs = NotImplemented # TODO\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        self.transcripts = NotImplemented\n",
        "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = NotImplemented\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = NotImplemented # TODO: Flatten to get 1d data\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C0rme6iT5dSb"
      },
      "outputs": [],
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "    pass\n",
        "\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJvMzHhB5dSc"
      },
      "outputs": [],
      "source": [
        "ROOT = \"\" # Define the root directory of the dataset here\n",
        "\n",
        "#TODO: Create a dataset object using the AudioDataset class for the training data\n",
        "train_data = NotImplemented\n",
        "\n",
        "# TODO: Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = NotImplemented\n",
        "\n",
        "# TODO: Create a dataset object using the AudioTestDataset class for the test data\n",
        "test_data = NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "# We shuffle train dataloader but not val & test dataloader. Why?\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Batch size     : \", config['batch_size'])\n",
        "print(\"Context        : \", config['context'])\n",
        "print(\"Input size     : \", (2*config['context']+1)*27)\n",
        "print(\"Output symbols : \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-GV3UvgLSoF"
      },
      "outputs": [],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "\n",
        "    # Visualize sample mfcc to inspect and verify everything is correctly done, especially augmentations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(frames[0].numpy().T, aspect='auto', origin='lower', cmap='viridis')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Features')\n",
        "    plt.title('Feature Representation')\n",
        "    plt.show()\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJTrLe7J5dSc"
      },
      "outputs": [],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "all = []\n",
        "for i, data in enumerate(val_loader):\n",
        "    frames, phoneme = data\n",
        "    all.append(phoneme)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-YsMpN-Exafq"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_qtrEM1ZvLje"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\n",
        "model       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device).cuda()\n",
        "\n",
        "# Check to stay below 20 MIL Parameter limit\n",
        "assert sum(p.numel() for p in model.parameters() if p.requires_grad) < 20_000_000, \"Exceeds 20 MIL params. Any submission made to Kaggle with this model will be flagged as an AIV.\"\n",
        "\n",
        "summary(model, frames.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZgQ7AgyTBnE"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "\n",
        "# Choose an appropriate optimizer of your choice\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "# Recommended : Define Scheduler for Learning Rate,\n",
        "# including but not limited to StepLR, MultiStep, CosineAnnealing, CosineAnnealingWithWarmRestarts, ReduceLROnPlateau, etc.\n",
        "# You can refer to Pytorch documentation for more information on how to use them.\n",
        "scheduler = NotImplemented\n",
        "\n",
        "# Is your training time very high?\n",
        "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "# Mixed Precision Training with AMP for speedup\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XblOHEVtKab2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # OPTIONAL: You can add gradient clipping here, if you face issues of exploding gradients\n",
        "\n",
        "        ### Gradient Descent\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCDYx5VEu6qI"
      },
      "outputs": [],
      "source": [
        "WANDB_API_KEY = \"YOUR_API_KEY\" # TODO: Replace with suitable wandb key for testing if you want.\n",
        "wandb.login(key=WANDB_API_KEY) #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvUnYd3Bw2up"
      },
      "outputs": [],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"CHANGE_THE_NAME\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"ID\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"MUST\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"HW1P2\", ### Project should be created in your wandb account\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wft15E_IxYFi"
      },
      "outputs": [],
      "source": [
        "### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model)\n",
        "\n",
        "### Save it in a txt file\n",
        "arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NNCA5DDTBnO"
      },
      "outputs": [],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc       = eval(model, val_loader)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "\n",
        "    ## Log metrics at each epoch in your run\n",
        "    # Optionally, you can log at each batch inside train/eval functions\n",
        "    # (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    # If using a scheduler, step the learning rate here, otherwise comment this line\n",
        "    # Depending on the scheduler in use, you may or may not need to pass in a metric into the step function, so read the docs well\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    ## Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijLrIJFl5dSf"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.____ # TODO train or eval?\n",
        "\n",
        "    ### List to store predicted phonemes of test data\n",
        "    test_predictions = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch._____: # TODO\n",
        "\n",
        "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            mfccs   = mfccs.to(device)\n",
        "\n",
        "            logits  = model(mfccs)\n",
        "\n",
        "            ### Get most likely predicted phoneme with argmax\n",
        "            predicted_phonemes = NotImplemented\n",
        "\n",
        "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
        "            # TODO\n",
        "\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG9v6Xmxu7wp"
      },
      "outputs": [],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMJMWCsXPI7A"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQQ3MWmNIiz"
      },
      "source": [
        "#### TODO: Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4QUk3UoEDIh"
      },
      "outputs": [],
      "source": [
        "import json, os, sys, torch, zipfile, datetime\n",
        "################################\n",
        "# TODO: Keep the model_metadata.json\n",
        "# file safe for submission later.\n",
        "################################\n",
        "def generate_model_submission_file():\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
        "    json_filename = f\"model_metadata_{timestamp}.json\"\n",
        "\n",
        "    # Create JSON with parameter count, model architecture, and predictions\n",
        "    output_json = {\n",
        "        \"parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "        \"model_architecture\": str(model),\n",
        "    }\n",
        "\n",
        "    # Save metadata JSON\n",
        "    with open(json_filename, \"w\") as f:\n",
        "        json.dump(output_json, f, indent=2)\n",
        "\n",
        "    # Download / display link depending on environment\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import files\n",
        "        files.download(json_filename)\n",
        "    elif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
        "        from IPython.display import FileLink, display\n",
        "        print(\"#\" * 100)\n",
        "        print(f\"Your submission file `{json_filename}` has been generated.\")\n",
        "        print(\"TODO: Click the link below.\")\n",
        "        print(\"1. The file will open in a new tab.\")\n",
        "        print(\"2. Right-click anywhere in the new tab and select 'Save As...'\")\n",
        "        print(\"3. Save the file to your computer with the `.json` extension.\")\n",
        "        print(\"You MUST submit this file to Autolab if this is your best submission.\")\n",
        "        print(\"#\" * 100 + \"\\n\")\n",
        "        display(FileLink(json_filename))\n",
        "    else:\n",
        "        print(f\"✅ saved model data saved to: '{json_filename}'\")\n",
        "        print(\"REQUIRED to submit to Autolab if these are the best model weights.\")\n",
        "\n",
        "generate_model_submission_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I6AVEY45dSg"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wf-P25TXU0N"
      },
      "outputs": [],
      "source": [
        "### Finish your wandb run\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjcammuCxMKN"
      },
      "outputs": [],
      "source": [
        "### Submit to kaggle competition using kaggle API\n",
        "\n",
        "!kaggle competitions submit -c 11785-hw-1-p-2-fall-25 -f submission.csv -m \"Test Submission\"\n",
        "\n",
        "### However, its always safer to download the csv file and then upload to kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28t6-Dff1LNv"
      },
      "source": [
        "# Final Code Submission Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxIHmPMj3vji"
      },
      "source": [
        "## TODO: fill in your submission requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqg6zK783t-N"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "#             README\n",
        "####################################\n",
        "\n",
        "# TODO: Please complete all components of this README\n",
        "# For TA-Testing feel free to leave this as is...\n",
        "README = \"\"\"\n",
        "- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n",
        "- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n",
        "- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n",
        "- **Notebook Execution**: Any instructions required to run your notebook.\n",
        "\"\"\"\n",
        "\n",
        "####################################\n",
        "#       Credentials (Optional)\n",
        "####################################\n",
        "\n",
        "# These are not required **IF** you have run the cells to declare these variables above.\n",
        "# If you would like to paste your credentials here again, feel free to:\n",
        "# OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables\n",
        "\n",
        "if \"KAGGLE_USERNAME\" not in globals():\n",
        "  KAGGLE_USERNAME = None # Optional: Put your kaggle username here\n",
        "\n",
        "if \"KAGGLE_API_KEY\" not in globals():\n",
        "  KAGGLE_API_KEY = None # Optional: Put your kaggle api key here\n",
        "\n",
        "if \"WANDB_API_KEY\" not in globals():\n",
        "  WANDB_API_KEY = None # Optional: Put your wandb api key here\n",
        "\n",
        "\n",
        "####################################\n",
        "#             Wandb Logs\n",
        "####################################\n",
        "\n",
        "# TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name\n",
        "#(Take these parameters and put them in the variables below)\n",
        "\n",
        "WANDB_USERNAME_OR_TEAMNAME = \"YOUR USER NAME\" # TODO: Put your username-or-team-name here\n",
        "WANDB_PROJECT = \"YOUR PROJECT NAME\" # TODO: Put your project-name\n",
        "\n",
        "####################################\n",
        "#         Notebook & Files\n",
        "####################################\n",
        "\n",
        "# TODO: Download HW1P2 Notebook (if on colab or kaggle) and upload both your HW1P2 notebook + model_metadata_*.json to your file system.\n",
        "# TODO: For each file, obtain the file paths and put them below.\n",
        "\n",
        "# TODO: COLAB INSTRUCTIONS:\n",
        "# * With Colab, upload your desired file (notebook or model_metadata.json) to \"Files\"\n",
        "# * Right-click the file, click \"Copy Path,\"\n",
        "# * Paste the path below.\n",
        "\n",
        "# TODO: KAGGLE INSTRUCTIONS:\n",
        "# * First download a copy of your notebook with \"File > Download Notebook\"\n",
        "# Then...\n",
        "# * Click \"File\" in the top left of the screen\n",
        "# * Go to \"Upload Input > Upload Model\"\n",
        "# * Upload your notebook file.\n",
        "# * For \"Model Name\" put HW2P2_Final_Submission\n",
        "# * For \"Framework\" put \"Other\"\n",
        "# * For \"License\" put \"Other\"\n",
        "# * Click \"Upload another file\" and upload your model_metadata####.json file as well.\n",
        "# * Now, on your right in your \"Models\" section, you should see a new folder with your submission files.\n",
        "# * Click on the \"Copy File Path\" buttons for the notebook and json file and paste them below.\n",
        "\n",
        "# TODO: Linux system:\n",
        "# * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.\n",
        "\n",
        "NOTEBOOK_PATH = \"/content/<YOUR_NOTEBOOK_PATH>.ipynb\" # TODO: Put your HW1P2 notebook path here\n",
        "MODEL_METADATA_JSON = \"/content/<YOUR_METADATA_JSON_PATH>.json\" # TODO: Put your Model Metadata path json file here (see end of HW1P2 Code Notebook to get this file)\n",
        "\n",
        "\n",
        "####################################\n",
        "#         Additional Files\n",
        "####################################\n",
        "\n",
        "ADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty\n",
        "]\n",
        "\n",
        "####################################\n",
        "#         SLACK SUBMISSION\n",
        "####################################\n",
        "\n",
        "ENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition\n",
        "\n",
        "####################################\n",
        "#     Creating the Submission\n",
        "####################################\n",
        "\n",
        "# TODO: Once the README, wandb information, and file paths are filled in, run this cell,\n",
        "# run the \"Assignment Backend Functions\" in the next cells, and generate the final zip file at the end.\n",
        "\n",
        "SAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmsS-WutY2n_"
      },
      "source": [
        "# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1MFM7JBcDba"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "######################################\n",
        "#       Assignment Configs\n",
        "######################################\n",
        "\n",
        "WANDB_METRIC = \"val_acc\"\n",
        "WANDB_DIRECTION = \"ascending\"\n",
        "WANDB_TOP_N = 10\n",
        "WANDB_OUTPUT_PKL = \"wandb_top_runs.pkl\"\n",
        "\n",
        "# Kaggle configuration\n",
        "COMPETITION_NAME = \"11785-hw-1-p-2-fall-25\"\n",
        "SLACK_COMPETITION_NAME = \"11785-hw-1-p-2-fall-25-slack\"\n",
        "FINAL_SUBMISSION_DATETIME = datetime.strptime(\"2025-09-19 23:55:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "SLACK_SUBMISSION_DATETIME = datetime.strptime(\"2025-10-03 23:55:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "GRADING_DIRECTION = \"ascending\"\n",
        "KAGGLE_OUTPUT_JSON = \"kaggle_data.json\"\n",
        "\n",
        "SUBMISSION_OUTPUT = \"HW1P2_final_submission.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JphgZ5NbjePX"
      },
      "outputs": [],
      "source": [
        "ACKNOWLEDGEMENT_MESSAGE = \"\"\"\n",
        "Submission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:\n",
        "Setting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:\n",
        "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
        "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
        "3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
        "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
        "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
        "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
        "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
        "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
        "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.\n",
        "\"\"\"\n",
        "def save_acknowledgment_file():\n",
        "    if ACKNOWLEDGED:\n",
        "        with open(\"acknowledgement.txt\", \"w\") as f:\n",
        "            f.write(ACKNOWLEDGEMENT_MESSAGE.strip())\n",
        "        print(\"Saved acknowledgement.txt\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "# Saves README\n",
        "def save_readme(readme):\n",
        "    try:\n",
        "        with open(\"README.txt\", \"w\") as f:\n",
        "            f.write(readme.strip())\n",
        "\n",
        "        print(\"Saved README.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error occured while saving README.txt: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "# Saves wandb logs\n",
        "import wandb, json, pickle\n",
        "\n",
        "def save_top_wandb_runs():\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "    if not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "\n",
        "    api = wandb.Api()\n",
        "    runs = api.runs(\n",
        "        f\"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}\",\n",
        "        order=f\"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}\"\n",
        "    )\n",
        "    selected_runs = runs[:min(WANDB_TOP_N, len(runs))]\n",
        "\n",
        "    if not selected_runs:\n",
        "        print(f\"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.\")\n",
        "        return False\n",
        "\n",
        "    all_data = []\n",
        "    for run in selected_runs:\n",
        "        run_data = {\n",
        "            \"id\": run.id,\n",
        "            \"name\": run.name,\n",
        "            \"tags\": run.tags,\n",
        "            \"state\": run.state,\n",
        "            \"created_at\": str(run.created_at),\n",
        "            \"config\": run.config,\n",
        "            \"summary\": dict(run.summary),\n",
        "        }\n",
        "        try:\n",
        "            run_data[\"history\"] = run.history(samples=1000)\n",
        "        except Exception as e:\n",
        "            run_data[\"history\"] = f\"Failed to fetch history: {str(e)}\"\n",
        "        all_data.append(run_data)\n",
        "    with open(WANDB_OUTPUT_PKL, \"wb\") as f:\n",
        "        pickle.dump(all_data, f)\n",
        "\n",
        "    print(f\"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}\")\n",
        "\n",
        "    return True\n",
        "# Saves kaggle information\n",
        "\n",
        "# Install dependencies silently (only if running on Colab)\n",
        "import sys\n",
        "\n",
        "from datetime import datetime\n",
        "import os, json, requests\n",
        "def kaggle_login(username, key):\n",
        "    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "    with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "        json.dump({\"username\": username, \"key\": key}, f)\n",
        "    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "\n",
        "def get_active_submission_config():\n",
        "    if ENABLE_SLACK_SUBMISSION:\n",
        "        return SLACK_COMPETITION_NAME, SLACK_SUBMISSION_DATETIME\n",
        "    return COMPETITION_NAME, FINAL_SUBMISSION_DATETIME\n",
        "\n",
        "def kaggle_user_exists(usernagbme):\n",
        "    try:\n",
        "        return requests.get(f\"https://www.kaggle.com/{KAGGLE_USERNAME}\").status_code == 200\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error while checking Kaggle user: {e}\")\n",
        "        return False\n",
        "\n",
        "DEFAULT_SCORE=0\n",
        "if GRADING_DIRECTION == \"ascending\":\n",
        "    DEFAULT_SCORE=0\n",
        "else:\n",
        "    DEFAULT_SCORE=1.0\n",
        "\n",
        "def get_best_kaggle_score(subs):\n",
        "    def extract_score(s): return float(s.private_score or s.public_score or DEFAULT_SCORE)\n",
        "    if not subs:\n",
        "        return None, None\n",
        "    best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == \"ascending\" else -extract_score(s))\n",
        "\n",
        "    score_type = \"private\" if best.private_score not in [None, \"\"] else \"public\"\n",
        "    return extract_score(best), score_type\n",
        "\n",
        "def save_kaggle_json(kaggle_username, kaggle_key):\n",
        "\n",
        "    kaggle_login(kaggle_username, kaggle_key)\n",
        "\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "    if not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "\n",
        "    if not kaggle_user_exists(KAGGLE_USERNAME):\n",
        "        print(f\"ERROR: User '{KAGGLE_USERNAME}' not found.\")\n",
        "        return False\n",
        "\n",
        "    comp_name, deadline = get_active_submission_config()\n",
        "\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    # Get competition submissions\n",
        "    submissions = [s for s in api.competition_submissions(comp_name) if getattr(s, \"submitted_by\", None) == KAGGLE_USERNAME and getattr(s, \"date\") <= deadline]\n",
        "    if not submissions:\n",
        "        print(f\"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]\")\n",
        "        print(\"Please double check your Kaggle username and ensure you've submitted at least once.\")\n",
        "        return False\n",
        "\n",
        "    score, score_type = get_best_kaggle_score(submissions)\n",
        "    result = {\n",
        "        \"kaggle_username\": KAGGLE_USERNAME,\n",
        "        \"acknowledgement\": ACKNOWLEDGED,\n",
        "        \"submitted_slack\": ENABLE_SLACK_SUBMISSION,\n",
        "        \"competition_name\": comp_name,\n",
        "        \"deadline\": deadline.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"raw_score\": score * 100.0,\n",
        "        \"score_type\": score_type,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}\")\n",
        "    if score:\n",
        "        print(f\"Best score {score}.\")\n",
        "        with open(KAGGLE_OUTPUT_JSON, \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def create_submission_zip(additional_files, safe_flag):\n",
        "    if not \"ACKNOWLEDGED\" in globals() or not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
        "        return\n",
        "\n",
        "    if (not save_acknowledgment_file()):\n",
        "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
        "        return\n",
        "\n",
        "    if not \"ENABLE_SLACK_SUBMISSION\" in globals() or ENABLE_SLACK_SUBMISSION is None:\n",
        "        print(\"ERROR: \\\"ENABLE_SLACK_SUBMISSION\\\" variable is not defined. \\nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \\nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \\n(if you are submitting through the SLACK submission).\")\n",
        "        return\n",
        "\n",
        "    if not \"README\" in globals() or not README:\n",
        "        print(\"ERROR: Make sure to RUN the README cell(above your credentials cell).\")\n",
        "        return\n",
        "\n",
        "    if (not save_readme(README)):\n",
        "        print(\"ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).\")\n",
        "        return\n",
        "\n",
        "    if (not save_top_wandb_runs()):\n",
        "        return\n",
        "\n",
        "    if not \"KAGGLE_USERNAME\" in globals() or not \"KAGGLE_API_KEY\" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:\n",
        "        print(\"ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.\")\n",
        "        return\n",
        "\n",
        "    if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):\n",
        "        print(f\"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.\")\n",
        "        return\n",
        "\n",
        "    files_to_zip = [\n",
        "        \"acknowledgement.txt\",\n",
        "        \"README.txt\",\n",
        "        KAGGLE_OUTPUT_JSON,\n",
        "        WANDB_OUTPUT_PKL,\n",
        "        MODEL_METADATA_JSON,\n",
        "        NOTEBOOK_PATH,\n",
        "    ] + additional_files\n",
        "\n",
        "    custom_missing_files_messages = {\n",
        "        KAGGLE_OUTPUT_JSON: \"ERROR: Kaggle data retrieval was missing, please check your kaggle username, API Key, and that you have submitted to the correct competition.\"\n",
        "    }\n",
        "\n",
        "    missing_files = False\n",
        "\n",
        "    with zipfile.ZipFile(SUBMISSION_OUTPUT, \"w\") as zipf:\n",
        "        for file_path in files_to_zip:\n",
        "            if os.path.exists(file_path):\n",
        "                arcname = os.path.basename(file_path)  # flatten path\n",
        "                zipf.write(file_path, arcname=arcname)\n",
        "                print(f\"OK: Added {arcname}\")\n",
        "            else:\n",
        "                missing_files = True\n",
        "                print(f\"ERROR: Missing file: {file_path}\")\n",
        "\n",
        "    if missing_files:\n",
        "        if safe_flag:\n",
        "            print(\"ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"WARNING: Missing files with safety flag set to False. Submission may be incomplete.\")\n",
        "\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import files\n",
        "        files.download(SUBMISSION_OUTPUT)\n",
        "\n",
        "    print(\"Final submission saved as:\", SUBMISSION_OUTPUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m2dFq8pc21u"
      },
      "source": [
        "# File Generation (TODO: Check file generation outputs for any errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii_kBJ7KZAYm"
      },
      "outputs": [],
      "source": [
        "create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)\n",
        "\n",
        "#TODO: If the HW1P2_final_submission.zip file does not\n",
        "# automatically bring up a donwload pop-up\n",
        "# Then make sure to entire the files and\n",
        "#manually download the checkpoint_submission.json file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 8695879,
          "sourceId": 80670,
          "sourceType": "competition"
        },
        {
          "datasetId": 6404483,
          "sourceId": 10342457,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
