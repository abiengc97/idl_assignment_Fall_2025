{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b9d3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "{'subset': 1.0, 'learning_rate': 0.001, 'epochs': 150, 'train_beam_width': 5, 'test_beam_width': 7, 'mfcc_features': 28, 'input_size': 28, 'embed_size': 256, 'batch_size': 128, 'encoder dropout': 0.2, 'lstm dropout': 0.2, 'decoder dropout': 0.2, 'wandb': True, 'freq_mask_param': 20, 'time_mask_param': 20, 'Name': 'Abiencherian', 'RUN_NAME': 'ablation3-higher masking after checkpoint from ab2'}\n",
      "Number of Phonemes: 41\n",
      "Index of Blank: 0\n",
      "Index of [SIL]: 1\n",
      "MFCC Shape: (1404, 28)\n",
      "\n",
      "MFCC:\n",
      " [[ 1.10582151e+01  1.28330450e+01  2.10689220e+01 ... -8.75406042e-02\n",
      "   7.30623007e-02  9.48924713e+01]\n",
      " [ 1.16850815e+01  1.38329487e+01  2.19087906e+01 ... -1.39180899e-01\n",
      "  -6.15991093e-02  9.55670319e+01]\n",
      " [ 1.11048431e+01  1.67276516e+01  2.28863831e+01 ... -4.03698802e-01\n",
      "   8.09250996e-02  9.54314651e+01]\n",
      " ...\n",
      " [ 2.62413955e+00 -1.26996317e+01  2.79472313e+01 ... -1.49060452e+00\n",
      "  -2.67574161e-01  1.16191261e+02]\n",
      " [ 3.05713993e-02 -1.20763254e+01  2.60116520e+01 ... -8.54688883e-01\n",
      "  -1.20284989e-01  1.17127373e+02]\n",
      " [-1.33604801e+00 -1.17101593e+01  2.44703121e+01 ... -9.30283189e-01\n",
      "  -8.16672146e-02  1.18280014e+02]]\n",
      "\n",
      "Transcript shape: (145,)\n",
      "\n",
      "Original Transcript:\n",
      " ['[SIL]' 'CH' 'AE' 'P' 'T' 'ER' 'W' 'AH' 'N' '[SIL]' 'M' 'IH' 'S' 'IH' 'Z'\n",
      " 'R' 'EY' 'CH' 'AH' 'L' 'IH' 'N' 'D' 'IH' 'Z' 'S' 'ER' 'P' 'R' 'AY' 'Z'\n",
      " 'D' '[SIL]' 'M' 'IH' 'S' 'IH' 'Z' 'R' 'EY' 'CH' 'AH' 'L' 'IH' 'N' 'D'\n",
      " '[SIL]' 'L' 'AY' 'V' 'D' '[SIL]' 'JH' 'AH' 'S' 'T' 'W' 'EH' 'R' 'DH' 'AH'\n",
      " 'AE' 'V' 'AH' 'N' 'L' 'IY' 'AH' 'M' 'EY' 'N' 'R' 'OW' 'D' '[SIL]' 'D'\n",
      " 'IH' 'P' 'T' '[SIL]' 'D' 'AW' 'N' 'IH' 'N' 'T' 'UW' 'AH' 'L' 'IH' 'T'\n",
      " 'AH' 'L' 'HH' 'AA' 'L' 'OW' '[SIL]' 'F' 'R' 'IH' 'N' 'JH' 'D' 'W' 'IH'\n",
      " 'DH' 'AA' 'L' 'D' 'ER' 'Z' 'AH' 'N' 'D' 'L' 'EY' 'D' 'IY' 'Z' 'IH' 'R'\n",
      " 'D' 'R' 'AA' 'P' 'S' 'AH' 'N' 'D' 'T' 'R' 'AE' 'V' 'ER' 'S' 'T' 'B' 'AY'\n",
      " 'AH' 'B' 'R' 'UH' 'K' '[SIL]']\n",
      "\n",
      "Transcript mapped from PHONEMES representation to LABELS representation:\n",
      " ['-', 'C', '@', 'p', 't', 'R', 'w', 'A', 'n', '-', 'm', 'I', 's', 'I', 'z', 'r', 'e', 'C', 'A', 'l', 'I', 'n', 'd', 'I', 'z', 's', 'R', 'p', 'r', 'Y', 'z', 'd', '-', 'm', 'I', 's', 'I', 'z', 'r', 'e', 'C', 'A', 'l', 'I', 'n', 'd', '-', 'l', 'Y', 'v', 'd', '-', 'j', 'A', 's', 't', 'w', 'E', 'r', 'D', 'A', '@', 'v', 'A', 'n', 'l', 'i', 'A', 'm', 'e', 'n', 'r', 'o', 'd', '-', 'd', 'I', 'p', 't', '-', 'd', 'W', 'n', 'I', 'n', 't', 'u', 'A', 'l', 'I', 't', 'A', 'l', 'h', 'a', 'l', 'o', '-', 'f', 'r', 'I', 'n', 'j', 'd', 'w', 'I', 'D', 'a', 'l', 'd', 'R', 'z', 'A', 'n', 'd', 'l', 'e', 'd', 'i', 'z', 'I', 'r', 'd', 'r', 'a', 'p', 's', 'A', 'n', 'd', 't', 'r', '@', 'v', 'R', 's', 't', 'b', 'Y', 'A', 'b', 'r', 'U', 'k', '-']\n",
      "\n",
      "Mapping list of PHONEMES to list of Integer indexes:\n",
      " {'': 0, '[SIL]': 1, 'NG': 2, 'F': 3, 'M': 4, 'AE': 5, 'R': 6, 'UW': 7, 'N': 8, 'IY': 9, 'AW': 10, 'V': 11, 'UH': 12, 'OW': 13, 'AA': 14, 'ER': 15, 'HH': 16, 'Z': 17, 'K': 18, 'CH': 19, 'W': 20, 'EY': 21, 'ZH': 22, 'T': 23, 'EH': 24, 'Y': 25, 'AH': 26, 'B': 27, 'P': 28, 'TH': 29, 'DH': 30, 'AO': 31, 'G': 32, 'L': 33, 'JH': 34, 'OY': 35, 'SH': 36, 'D': 37, 'AY': 38, 'S': 39, 'IH': 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28539/28539 [00:08<00:00, 3239.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2703/2703 [00:00<00:00, 4021.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  128\n",
      "Train dataset samples = 28539, batches = 223\n",
      "Val dataset samples = 2703, batches = 22\n",
      "Test dataset samples = 2620, batches = 21\n",
      "torch.Size([128, 1668, 28]) torch.Size([128, 205]) torch.Size([128]) torch.Size([128])\n",
      "Network(\n",
      "  (embedding): Conv1d(28, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (lstm): LSTM(28, 256)\n",
      "  (classification): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=41, bias=True)\n",
      "  )\n",
      "  (logSoftmax): LogSoftmax(dim=2)\n",
      ")\n",
      "torch.Size([128, 367, 41])\n",
      "torch.Size([367, 128, 41]) torch.Size([128, 265])\n",
      "tensor(3.9188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "60.609375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import wandb\n",
    "import torchaudio.transforms as tat\n",
    "from torchaudio.models.decoder import cuda_ctc_decoder\n",
    "import Levenshtein\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import glob\n",
    "\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "\n",
    "Name = \"Abiencherian\" # Write your name here\n",
    "import yaml\n",
    "with open(\"/home/agcheria/idl_assignment_Fall_2025_1/HW3/HW3P2/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "print(config)\n",
    "\n",
    "BATCH_SIZE = config[\"batch_size\"] # Define batch size from config\n",
    "root = \"/home/agcheria/idl_assignment_Fall_2025_1/HW3/HW3P2/11785-hw3p2\" # Specify the directory to your root based on your environment: Google Colab, Kaggle, or PSC\n",
    "\n",
    "# ARPABET PHONEME MAPPING\n",
    "# DO NOT CHANGE\n",
    "\n",
    "CMUdict_ARPAbet = {\n",
    "    \"\" : \" \",\n",
    "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
    "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
    "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
    "    \"HH\"   : \"h\", \"Z\" :\n",
    "     \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
    "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
    "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
    "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
    "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
    "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
    "}\n",
    "\n",
    "\n",
    "CMUdict = list(CMUdict_ARPAbet.keys())\n",
    "ARPAbet = list(CMUdict_ARPAbet.values())\n",
    "\n",
    "PHONEMES = CMUdict[:-2] #To be used for mapping original transcripts to integer indices\n",
    "LABELS = ARPAbet[:-2] #To be used for mapping predictions to strings\n",
    "\n",
    "OUT_SIZE = len(PHONEMES) # Number of output classes\n",
    "print(\"Number of Phonemes:\", OUT_SIZE)\n",
    "\n",
    "# Indexes of BLANK and SIL phonemes\n",
    "BLANK_IDX=CMUdict.index('')\n",
    "SIL_IDX=CMUdict.index('[SIL]')\n",
    "\n",
    "print(\"Index of Blank:\", BLANK_IDX)\n",
    "print(\"Index of [SIL]:\", SIL_IDX)\n",
    "\n",
    "test_mfcc = f\"{root}/train-clean-100/mfcc/103-1240-0000.npy\"\n",
    "test_transcript = f\"{root}/train-clean-100/transcript/103-1240-0000.npy\"\n",
    "\n",
    "mfcc = np.load(test_mfcc)\n",
    "transcript = np.load(test_transcript)[1:-1] #Removed [SOS] and [EOS]\n",
    "\n",
    "print(\"MFCC Shape:\", mfcc.shape)\n",
    "print(\"\\nMFCC:\\n\", mfcc)\n",
    "print(\"\\nTranscript shape:\", transcript.shape)\n",
    "\n",
    "print(\"\\nOriginal Transcript:\\n\", transcript)\n",
    "\n",
    "# map the loaded transcript (from phonemes representation) to corresponding labels representation\n",
    "mapped_transcript = [CMUdict_ARPAbet[k] for k in transcript]\n",
    "print(\"\\nTranscript mapped from PHONEMES representation to LABELS representation:\\n\", mapped_transcript)\n",
    "\n",
    "# Mapping list of PHONEMES to list of Integer indexes\n",
    "map = {k: i for i, k in enumerate(PHONEMES)}\n",
    "print(\"\\nMapping list of PHONEMES to list of Integer indexes:\\n\", map)\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    # Hint: The data from HW1 is very similar to this HW\n",
    "\n",
    "    #TODO\n",
    "    def __init__(self, root, partition=\"train-clean-100\", train=True, freq_mask_param=10, time_mask_param=10):\n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "\n",
    "        self.PHONEMES = PHONEMES\n",
    "        self.subset = config['subset']\n",
    "        self.train = train  # Flag to control augmentation\n",
    "        self.freq_masking = tat.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
    "        self.time_masking = tat.TimeMasking(time_mask_param=time_mask_param)\n",
    "\n",
    "        # TODO\n",
    "        # Define the directories containing MFCC and transcript files\n",
    "        self.mfcc_dir = os.path.join(root, partition, 'mfcc')\n",
    "        self.transcript_dir = os.path.join(root, partition, 'transcript')\n",
    "\n",
    "        # List all files in the directories. Remember to sort the files\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)  )\n",
    "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
    "\n",
    "        # Compute size of data subset\n",
    "        subset_size = int(self.subset * len(self.mfcc_files))\n",
    "\n",
    "        # Select subset of data to use\n",
    "        self.mfcc_files = self.mfcc_files[:subset_size]\n",
    "        self.transcript_files = self.transcript_files[:subset_size]\n",
    "\n",
    "        assert(len(self.mfcc_files) == len(self.transcript_files))\n",
    "\n",
    "        #TODO\n",
    "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
    "        self.length = len(self.mfcc_files)\n",
    "\n",
    "        #TODO\n",
    "        # CREATE AN ARRAY TO STORE ALL PROCESSED MFCCS AND TRANSCRIPTS\n",
    "        # LOAD ALL MFCCS AND CORRESPONDING TRANSCRIPTS AND DO THE NECESSARY PRE-PROCESSING\n",
    "          # HINTS:\n",
    "          # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
    "          # REMEMBER TO REMOVE [SOS] AND [EOS] FROM TRANSCRIPTS\n",
    "        '''\n",
    "        You may decide to do this in __getitem__ if you wish.\n",
    "        However, doing this here will make the __init__ function take the load of\n",
    "        loading the data, and shift it away from training.\n",
    "        '''\n",
    "        self.mfccs = []\n",
    "        self.transcripts = []\n",
    "        for i in tqdm(range(len(self.mfcc_files))):\n",
    "\n",
    "            # TODO: Load a single mfcc. Hint: Use numpy\n",
    "            mfcc             = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
    "            # TODO: Do Cepstral Normalization of mfcc along the Time Dimension (Think about the correct axis)\n",
    "            mfccs_normalized = (mfcc - np.mean(mfcc, axis=0)) /( np.std(mfcc, axis=0)+1e-6)\n",
    "\n",
    "            # Convert mfcc to tensor\n",
    "            mfccs_normalized = torch.tensor(mfccs_normalized, dtype=torch.float32)\n",
    "\n",
    "            # TODO: Load the corresponding transcript\n",
    "            # Remove [SOS] and [EOS] from the transcript\n",
    "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
    "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
    "            transcript = np.load(f'{self.transcript_dir}/{self.transcript_files[i]}')[1:-1]\n",
    "            # The available phonemes in the transcript are of string data type\n",
    "            # But the neural network cannot predict strings as such.\n",
    "            # Hence, we map these phonemes to integers\n",
    "\n",
    "            # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
    "            transcript_indices = [self.PHONEMES.index(phoneme) for phoneme in transcript]\n",
    "            # Now, if an element in the transcript is 0, it means that it is 'SIL' (as per the above example)\n",
    "\n",
    "            # Convert transcript to tensor\n",
    "            transcript_indices = torch.tensor(transcript_indices, dtype=torch.int64)\n",
    "\n",
    "            # Append each mfcc to self.mfcc, transcript to self.transcript\n",
    "            self.mfccs.append(mfccs_normalized)\n",
    "            self.transcripts.append(transcript_indices)\n",
    "        \n",
    "\n",
    "        #TODO\n",
    "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
    "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
    "        self.map = {k: i for i, k in enumerate(PHONEMES)}\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        '''\n",
    "        TODO: What do we return here?\n",
    "        '''\n",
    "\n",
    "        return self.length\n",
    "\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "\n",
    "        '''\n",
    "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
    "\n",
    "        If you didn't do the loading and processing of the data in __init__,\n",
    "        do that here.\n",
    "\n",
    "        Once done, return a tuple of features and labels.\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Use preloaded and normalized tensors from __init__\n",
    "        return self.mfccs[ind], self.transcripts[ind]\n",
    "\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish.\n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features,\n",
    "            and lengths of labels.\n",
    "        '''\n",
    "\n",
    "        # Extract batch of input MFCCs and batch of output transcripts separately\n",
    "        batch_mfcc = [item[0] for item in batch]\n",
    "        batch_transcript = [item[1] for item in batch]\n",
    "\n",
    "        # Store original lengths of the MFCCS and transcripts in the batches\n",
    "        lengths_mfcc = [item.shape[0] for item in batch_mfcc]\n",
    "        lengths_transcript = [item.shape[0] for item in batch_transcript]\n",
    "\n",
    "        # Apply SpecAugment BEFORE padding, and only on training data\n",
    "        if self.train:\n",
    "            aug_mfcc = []\n",
    "            for mfcc in batch_mfcc:\n",
    "                # mfcc shape: (T, F) -> need (1, F, T) for torchaudio transforms\n",
    "                mfcc_aug = mfcc.T.unsqueeze(0)  # (1, F, T)\n",
    "                mfcc_aug = self.time_masking(self.freq_masking(mfcc_aug)).squeeze(0)  # (F, T)\n",
    "                aug_mfcc.append(mfcc_aug.T)  # back to (T, F)\n",
    "            batch_mfcc = aug_mfcc\n",
    "\n",
    "        # Pad the MFCC sequences and transcripts\n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        # Note: (resulting shape of padded MFCCs: [batch, time, freq])\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
    "\n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
    "    \n",
    "    \n",
    "    # TODO\n",
    "# Food for thought -> Do you need to apply transformations in this test dataset class?\n",
    "class AudioDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, partition=\"test-clean\"):\n",
    "        self.root = root\n",
    "        self.partition = partition\n",
    "        self.mfcc_dir = os.path.join(root, partition, 'mfcc')\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
    "        self.length = len(self.mfcc_files)\n",
    "        self.mfccs = []\n",
    "        for mfcc_file in self.mfcc_files:\n",
    "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_file))\n",
    "            mfcc=(mfcc-np.mean(mfcc, axis=0))/(np.std(mfcc, axis=0) + 1e-6)\n",
    "            self.mfccs.append(torch.tensor(mfcc, dtype=torch.float32))\n",
    "        self.map = {k: i for i, k in enumerate(PHONEMES)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        mfcc = self.mfccs[ind]\n",
    "        return mfcc\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        '''\n",
    "        Collate function for test dataset to handle variable-length MFCC sequences.\n",
    "        Returns padded MFCCs and their lengths.\n",
    "        '''\n",
    "        # Extract batch of MFCCs\n",
    "        batch_mfcc = batch\n",
    "        \n",
    "        # Store original lengths of the MFCCs\n",
    "        lengths_mfcc = [item.shape[0] for item in batch_mfcc]\n",
    "        \n",
    "        # Pad the MFCC sequences\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
    "        \n",
    "        # Return padded features and actual lengths\n",
    "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# To free up ram\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Create objects for the dataset classes\n",
    "# Progressive augmentation curriculum: start with light augmentation, increase later\n",
    "freq_mask = config.get('freq_mask_param', 10)\n",
    "time_mask = config.get('time_mask_param', 10)\n",
    "train_data = AudioDataset(root=root, partition=\"train-clean-100\", train=True, freq_mask_param=freq_mask, time_mask_param=time_mask)\n",
    "val_data = AudioDataset(root=root, partition=\"dev-clean\", train=False)\n",
    "test_data = AudioDatasetTest(root=root, partition=\"test-clean\")\n",
    "\n",
    "# Do NOT forget to pass in the collate function as an argument while creating the dataloader\n",
    "train_loader = DataLoader(train_data,num_workers=4, pin_memory=True, shuffle=True, batch_size=config['batch_size'], collate_fn=train_data.collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_data, num_workers=0, pin_memory=True, shuffle=False, batch_size=config['batch_size'], collate_fn=val_data.collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_data, num_workers=4, pin_memory=True, shuffle=False, batch_size=config['batch_size'], collate_fn=test_data.collate_fn)\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n",
    "\n",
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # TODO: Adding some sort of embedding layer or feature extractor might help performance.\n",
    "        # You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
    "        # Food for thought -> What type of Conv layers can be used here?\n",
    "        #                  -> What should be the size of input channels to the first layer?\n",
    "        self.embedding = nn.Conv1d(in_channels=config['input_size'], out_channels=config['embed_size'], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
    "        self.lstm = nn.LSTM(input_size = config['input_size'], hidden_size = config['embed_size'], num_layers = 1) #TODO\n",
    "\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(config['embed_size']  , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, OUT_SIZE),\n",
    "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
    "        )\n",
    "\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, lx):   \n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.classification(x)\n",
    "        x = self.logSoftmax(x)\n",
    "        return x\n",
    "        #TODO\n",
    "        # The forward function takes 2 parameter inputs here. Why?\n",
    "        # Refer to the handout for hints\n",
    "        \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = Network().to(device)\n",
    "# Check to stay below 20 MIL Parameter limit\n",
    "# assert sum(p.numel() for p in model.parameters() if p.requires_grad) < 20_000_000, \"Exceeds 20 MIL params. Any submission made to Kaggle with this model will be flagged as an AIV.\"\n",
    "\n",
    "print(model)\n",
    "        \n",
    "class Permute(torch.nn.Module):\n",
    "    '''\n",
    "    Used to transpose/permute the dimensions of an MFCC tensor.\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "class pBLSTM(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
    "\n",
    "    At each step,\n",
    "    1. Pad your input if it is packed (Unpack it)\n",
    "    2. Reduce the input length dimension by concatenating feature dimension\n",
    "        (Tip: Write down the shapes and understand)\n",
    "        (i) How should  you deal with odd/even length input?\n",
    "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
    "    3. Pack your input\n",
    "    4. Pass it into LSTM layer\n",
    "\n",
    "    To make our implementation modular, we pass 1 layer at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.blstm = nn.LSTM(\n",
    "            input_size=input_size * 2,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
    "\n",
    "        # TODO: Pad Packed Sequence\n",
    "        x_unpacked, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
    "\n",
    "        # TODO: Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
    "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
    "        x, x_lens = self.trunc_reshape(x_unpacked, x_lens)\n",
    "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
    "        x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False) \n",
    "        # TODO: Pass the sequence through bLSTM\n",
    "        x_packed, _ = self.blstm(x_packed)\n",
    "        \n",
    "        # What do you return?\n",
    "\n",
    "        return x_packed\n",
    "\n",
    "    def trunc_reshape(self, x, x_lens):\n",
    "\n",
    "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
    "        # Truncate to even number of timesteps\n",
    "        B, T, F = x.shape\n",
    "        if T % 2 == 1:\n",
    "            x = x[:, :-1, :]\n",
    "            x_lens = x_lens - 1\n",
    "        \n",
    "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
    "        x = x.reshape(B, T // 2, 2 * F)\n",
    "\n",
    "        \n",
    "        # TODO: Reduce lengths by the same downsampling factor\n",
    "        x_lens = x_lens // 2\n",
    "\n",
    "        return x, x_lens\n",
    "\n",
    "class LSTMWrapper(torch.nn.Module):\n",
    "    '''\n",
    "    Used to get only output of lstm, not the hidden states.\n",
    "    '''\n",
    "    def __init__(self, lstm):\n",
    "        super(LSTMWrapper, self).__init__()\n",
    "        self.lstm = lstm\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    '''\n",
    "    The Encoder takes utterances as inputs and returns latent feature representations\n",
    "    '''\n",
    "    def __init__(self, input_size, encoder_hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "\n",
    "        # TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
    "        # Food for thought -> What type of Conv layers can be used here?\n",
    "        #                  -> What should be the size of input channels to the first layer?\n",
    "        self.embedding = nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # TODO:\n",
    "        self.BLSTMs = nn.LSTM(\n",
    "            # TODO: Look up the documentation. You might need to pass some additional parameters.\n",
    "            input_size=128,\n",
    "            hidden_size=encoder_hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "          )\n",
    "\n",
    "        self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
    "            # TODO: Fill this up with pBLSTMs - What should the input_size be?\n",
    "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
    "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
    "            # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
    "            # ...\n",
    "            pBLSTM(input_size=2*encoder_hidden_size, hidden_size=encoder_hidden_size),  # 512 input (2*256)\n",
    "            pBLSTM(input_size=2*encoder_hidden_size, hidden_size=encoder_hidden_size),  # 512 input (2*256)\n",
    "            pBLSTM(input_size=2*encoder_hidden_size, hidden_size=encoder_hidden_size),  # 512 input (2*256)\n",
    "            \n",
    "            # ...\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        # Where are x and x_lens coming from? The dataloader\n",
    "\n",
    "        # TODO: Call the embedding layer\n",
    "        x=x.transpose(1, 2)\n",
    "        x=self.embedding(x)\n",
    "        x=x.transpose(1, 2)\n",
    "\n",
    "        # TODO: Pack Padded Sequence\n",
    "        x_packed=pack_padded_sequence(x, x_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # TODO: Pass Sequence through the Bi-LSTM layer\n",
    "        x_packed, _ = self.BLSTMs(x_packed)\n",
    "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
    "        for layer in self.pBLSTMs:\n",
    "            x_packed = layer(x_packed)\n",
    "        # TODO: Pad Packed Sequence\n",
    "\n",
    "        # Remember the number of output(s) each function returns\n",
    "        \n",
    "        # Pack the sequence before passing to pBLSTMs (lengths need to be on CPU)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_packed, batch_first=True)\n",
    "\n",
    "        # After 3 pBLSTMs, the time dimension is reduced by 8Ã—\n",
    "        x_lens = x_lens // (2 ** len(self.pBLSTMs))\n",
    "        # Clamp to ensure lengths are at least 1 (avoid errors with very short sequences)\n",
    "        x_lens = torch.clamp(x_lens, min=1)\n",
    "\n",
    "        return x_unpacked, x_lens\n",
    "    \n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, output_size= 41):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "\n",
    "            Permute(),\n",
    "            torch.nn.BatchNorm1d(2 * embed_size),\n",
    "            Permute(),\n",
    "\n",
    "            #TODO define your MLP arch. Refer HW1P2\n",
    "            #Use Permute Block before and after BatchNorm1d() to match the size\n",
    "            #Now you can stack your MLP layers\n",
    "            nn.Linear(2 * embed_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, output_size),\n",
    "        )\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "\n",
    "        #TODO: Call your MLP\n",
    "\n",
    "        #TODO: Think about what should be the final output of the decoder for classification\n",
    "        out = self.mlp(encoder_out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder        = Encoder(input_size, embed_size)\n",
    "        self.decoder        = Decoder(embed_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths_x):\n",
    "\n",
    "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
    "        decoder_out                 = self.decoder(encoder_out)\n",
    "\n",
    "        return decoder_out, encoder_lens\n",
    "    \n",
    "model = ASRModel(\n",
    "    input_size  = config['input_size'],\n",
    "    embed_size  = config['embed_size'],\n",
    "    output_size = len(PHONEMES)\n",
    ").to(device)\n",
    "\n",
    "# Check to stay below 20 MIL Parameter limit\n",
    "# assert sum(p.numel() for p in model.parameters() if p.requires_grad) < 20_000_000, \"Exceeds 20 MIL params. Any submission made to Kaggle with this model will be flagged as an AIV.\"\n",
    "\n",
    "summary(model, input_data=[x.to(device), lx.to(device)])\n",
    "\n",
    "\n",
    "# TODO: Define CTC loss as the criterion. How would the losses be reduced?\n",
    "criterion = nn.CTCLoss(\n",
    "    blank=BLANK_IDX,      # your \"\" token index\n",
    "    reduction=\"mean\",\n",
    "    zero_infinity=True\n",
    ")\n",
    "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "# Refer to the handout for hints\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=1e-2\n",
    ") #TODO: What goes in here?\n",
    "\n",
    "# TODO: Declare the decoder. Use the PyTorch Cuda CTC Decoder to decode phonemes\n",
    "# CTC Decoder: https://pytorch.org/audio/2.1/generated/torchaudio.models.decoder.cuda_ctc_decoder.html\n",
    "decoder = cuda_ctc_decoder(\n",
    "    tokens=LABELS,                         # same class order as the network output\n",
    "    nbest=1,\n",
    "    beam_size=config[\"train_beam_width\"],\n",
    "    blank_skip_threshold=0.95\n",
    ") #TODO\n",
    "\n",
    "# TODO:\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config[\"learning_rate\"],\n",
    "    epochs=config[\"epochs\"],\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy=\"cos\"\n",
    ")\n",
    "\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_prediction(output, output_lens, decoder, PHONEME_MAP = LABELS):\n",
    "\n",
    "    # Look at docs for CUDA_CTC_DECODER for more info on how it was used here:\n",
    "    # https://pytorch.org/audio/main/tutorials/asr_inference_with_cuda_ctc_decoder_tutorial.html\n",
    "    output = output.contiguous()\n",
    "    output_lens = output_lens.to(torch.int32).contiguous()\n",
    "    beam_results = decoder(output, output_lens.to(torch.int32)) #lengths - list of lengths\n",
    "\n",
    "    pred_strings = []\n",
    "\n",
    "    for i in range(len(beam_results)):\n",
    "        # Robustly handle different decoder return types\n",
    "        hyp0 = beam_results[i][0]\n",
    "        \n",
    "        # Try attribute access first, then dict-like access\n",
    "        tokens = getattr(hyp0, \"tokens\", None)\n",
    "        if tokens is None:\n",
    "            tokens = hyp0.get(\"tokens\", None) if hasattr(hyp0, \"get\") else None\n",
    "        \n",
    "        # Convert to list if it's a tensor\n",
    "        if torch.is_tensor(tokens):\n",
    "            tokens = tokens.tolist()\n",
    "        \n",
    "        # Map the sequence of indices to actual phoneme LABELS and join them into a string\n",
    "        pred_strings.append(\"\".join(PHONEME_MAP[t] for t in tokens))\n",
    "    return pred_strings\n",
    "\n",
    "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
    "\n",
    "    dist            = 0\n",
    "    batch_size      = label.shape[0]\n",
    "\n",
    "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Truncate labels by their true lengths (not padded length)\n",
    "        Li = int(label_lens[i].item())\n",
    "        lab = label[i, :Li].tolist()\n",
    "        label_string = \"\".join(PHONEME_MAP[t] for t in lab)\n",
    "        pred_string = pred_strings[i]\n",
    "\n",
    "        dist += Levenshtein.distance(pred_string, label_string)\n",
    "\n",
    "    # Average the distance over the batch\n",
    "    dist /= batch_size # Think about why we are doing this\n",
    "    return dist\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# test code to check shapes\n",
    "\n",
    "model.eval()\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    x, y, lx, ly = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    lx, ly = lx.to(device), ly.to(device)\n",
    "    h, lh = model(x, lx)\n",
    "    print(h.shape)\n",
    "    h = torch.permute(h, (1, 0, 2))\n",
    "    print(h.shape, y.shape)\n",
    "    loss = criterion(h, y, lh, ly)\n",
    "    print(loss)\n",
    "\n",
    "    print(calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh.to(device), ly, decoder, LABELS))\n",
    "\n",
    "    del x, y, lx, ly, h, lh, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# # Use wandb? Resume Training?\n",
    "# USE_WANDB = config['wandb']\n",
    "\n",
    "# RESUME_LOGGING = False # Set this to true if you are resuming training from a previous run\n",
    "\n",
    "# # Create your wandb run\n",
    "# run_name = '{RUN_NAME}_checkpoint_submission'.format(RUN_NAME=config['RUN_NAME'])\n",
    "\n",
    "# # If you are resuming an old run\n",
    "# if USE_WANDB:\n",
    "\n",
    "#     wandb.login(key=\"\") #TODO\n",
    "\n",
    "#     if RESUME_LOGGING:\n",
    "#         run = wandb.init(\n",
    "#             id     = \"\", ### Insert specific run id here if you want to resume a previous run\n",
    "#             resume = \"must\", ### You need this to resume previous runs\n",
    "#             project = \"hw3p2-ablations\", ### Project should be created in your wandb\n",
    "#             settings = wandb.Settings(_service_wait=300)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     else:\n",
    "#         run = wandb.init(\n",
    "#             name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "#             reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "#             project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
    "#             config  = config ### Wandb Config for your run\n",
    "#         )\n",
    "\n",
    "#         ### Save your model architecture as a string with str(model)\n",
    "#         model_arch  = str(model)\n",
    "#         ### Save it in a txt file\n",
    "#         arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "#         file_write  = arch_file.write(model_arch)\n",
    "#         arch_file.close()\n",
    "\n",
    "#         ### log it in your wandb run with wandb.save()\n",
    "#         wandb.save('model_arch.txt')\n",
    "# Train function\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        lx, ly = lx.to(device), ly.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        \n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "        \n",
    "        # Step scheduler per batch for OneCycleLR\n",
    "        scheduler.step()\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Eval function\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        lx, ly = lx.to(device), ly.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh.to(device), ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist\n",
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict() if scheduler is not None else {},\n",
    "         metric[0]                  : metric[1],\n",
    "         'epoch'                    : epoch},\n",
    "         path\n",
    "    )\n",
    "    print(f\"âœ“ Checkpoint saved locally to: {path}\")\n",
    "\n",
    "def load_model(path, model, optimizer= None, scheduler= None, metric='valid_dist'):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    print(\"\\nResuming training from epoch:\", epoch)\n",
    "    print('----------------------------------------\\n')\n",
    "    print(\"Epochs left: \", config['epochs'] - epoch)\n",
    "    print(\"Optimizer: \\n\", optimizer)\n",
    "    print(\"Current Schedueler T_cur:\", scheduler.T_cur)\n",
    "\n",
    "    print(\"Best Val Dist:\", metric)\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]\n",
    "#load a checkpoint /home/agcheria/idl_assignment_Fall_2025_1/checkpoints/checkpoint-best-model.pth\n",
    "checkpoint = torch.load('/home/agcheria/idl_assignment_Fall_2025_1/HW3/HW3P2/checkpoints_2best/checkpoint-best-model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Instantiate variables used in training loop\n",
    "last_epoch_completed = 0\n",
    "best_lev_dist = float(\"inf\")\n",
    "\n",
    "# # RESUME_TRAINING = True # Set this to true if you are resuming training from a mpdel checkpoint\n",
    "\n",
    "# # if RESUME_TRAINING:\n",
    "\n",
    "# #     checkpoint_path = ''\n",
    "# #     checkpoint = load_model(checkpoint_path, model, optimizer, scheduler, metric='valid_dist')\n",
    "\n",
    "# #     last_epoch_completed = checkpoint[3]\n",
    "# #     best_lev_dist = checkpoint[4]\n",
    "\n",
    "# # Set up checkpoint directories and WanDB logging watch\n",
    "# checkpoint_root = os.path.join(os.getcwd(), 'checkpoints')\n",
    "# os.makedirs(checkpoint_root, exist_ok=True)\n",
    "# wandb.watch(model, log=\"all\")\n",
    "\n",
    "# checkpoint_best_model_filename = 'checkpoint-best-model.pth'\n",
    "# checkpoint_last_epoch_filename = 'checkpoint-last-epoch.pth'\n",
    "# epoch_model_path = os.path.join(checkpoint_root, checkpoint_last_epoch_filename)\n",
    "# best_model_path = os.path.join(checkpoint_root, checkpoint_best_model_filename)\n",
    "\n",
    "# # # WanDB log watch\n",
    "# # if config['wandb']:\n",
    "# #   wandb.watch(model, log=\"all\")\n",
    "\n",
    "\n",
    "# # # Clear RAM for storage before you start training\n",
    "# # torch.cuda.empty_cache()\n",
    "# # gc.collect()\n",
    "\n",
    "#TODO: Please complete the training loop\n",
    "\n",
    "# for epoch in range(last_epoch_completed, config['epochs']):\n",
    "\n",
    "#     print(\"\\nEpoch: {}/{}\".format(epoch + 1, config['epochs']))\n",
    "\n",
    "#     curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "#     train_loss = train_model(model, train_loader, criterion, optimizer)\n",
    "#     valid_loss, valid_dist = validate_model(model, val_loader, decoder)\n",
    "\n",
    "#     # NOTE: OneCycleLR is stepped per batch inside train_model, not here\n",
    "\n",
    "#     print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "#     print(\"\\tVal Dist {:.04f}\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
    "\n",
    "#     if config['wandb']:\n",
    "#         wandb.log({\n",
    "#             'train_loss': train_loss,\n",
    "#             'valid_dist': valid_dist,\n",
    "#             'valid_loss': valid_loss,\n",
    "#             'lr': curr_lr\n",
    "#     })\n",
    "\n",
    "#     # Save last epoch model locally\n",
    "#     save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
    "#     if config['wandb']:\n",
    "#         wandb.save(epoch_model_path)  # Upload to wandb\n",
    "\n",
    "#     # Save best model when validation improves\n",
    "#     if valid_dist <= best_lev_dist:\n",
    "#         best_lev_dist = valid_dist\n",
    "#         save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
    "#         if config['wandb']:\n",
    "#             wandb.save(best_model_path)  # Upload to wandb\n",
    "#         print(f\"ðŸŽ¯ New best model! (dist: {valid_dist:.4f})\")\n",
    "\n",
    "# # You may find it interesting to explore Wandb Artifacts to version your models\n",
    "\n",
    "# # Finish Wandb run\n",
    "# if config['wandb']:\n",
    "#     run.finish()\n",
    "#load checkpoint\n",
    "checkpoint = torch.load('/home/agcheria/idl_assignment_Fall_2025_1/HW3/HW3P2/checkpoints_2best/checkpoint-best-model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9056732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:04<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Make predictions\n",
    "\n",
    "# Follow the steps below:\n",
    "# 1. Create a new object for CUDA_CTC_DECODER with larger number of beams (why larger?)\n",
    "# 2. Get prediction string by decoding the results of the beam decoder\n",
    "\n",
    "\n",
    "test_decoder = cuda_ctc_decoder(\n",
    "    tokens=LABELS,\n",
    "    nbest=1,\n",
    "    beam_size=6,  \n",
    "    blank_skip_threshold=0.95\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "torch.backends.cudnn.benchmark = True  # speed up conv/LSTM for fixed shapes\n",
    "\n",
    "results = []\n",
    "with torch.inference_mode():\n",
    "    for i, (x, lx) in enumerate(tqdm(test_loader)):\n",
    "        x  = x.to(device, non_blocking=True)\n",
    "        lx = lx.to(device, non_blocking=True)\n",
    "\n",
    "        h, lh = model(x, lx)                    # h: (B, T, C) log-probs; lh: (B,)\n",
    "        \n",
    "        preds = decode_prediction(h, lh, test_decoder, LABELS)\n",
    "        results.extend(preds)\n",
    "        \n",
    "        # Clear memory after each batch\n",
    "        del x, lx, h, lh\n",
    "        torch.cuda.empty_cache()      \n",
    "        \n",
    "if results:\n",
    "    df = pd.DataFrame({\n",
    "        'index': range(len(results)), 'label': results\n",
    "    })\n",
    "\n",
    "data_dir = \"submission.csv\"\n",
    "df.to_csv(data_dir, index = False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
